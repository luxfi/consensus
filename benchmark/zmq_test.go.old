// Copyright (C) 2019-2025, Lux Industries Inc. All rights reserved.
// See the file LICENSE for licensing terms.

//go:build zmq
// +build zmq

package benchmark_test

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"sync/atomic"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	"github.com/luxfi/consensus/config"
	"github.com/luxfi/consensus/snowball"
	"github.com/luxfi/consensus/transport"
	"github.com/luxfi/consensus/transport/zmq"
	"github.com/luxfi/ids"
)

var _ = Describe("ZeroMQ Transport Benchmarks", Ordered, func() {
	var (
		nodes      []*ConsensusNode
		ctx        context.Context
		cancel     context.CancelFunc
		basePort   = 20000
		msgCounter atomic.Int64
		tpsCounter atomic.Int64
	)

	BeforeAll(func() {
		ctx, cancel = context.WithCancel(context.Background())
		DeferCleanup(cancel)
	})

	Context("Massively Parallel Consensus", func() {
		It("should achieve high TPS with 5 local validators", func() {
			nodeCount := 5
			params := config.HighTPSParams
			
			By(fmt.Sprintf("Creating %d validator nodes", nodeCount))
			nodes = createNodes(nodeCount, basePort, params)
			
			By("Starting all nodes")
			var wg sync.WaitGroup
			for _, node := range nodes {
				wg.Add(1)
				go func(n *ConsensusNode) {
					defer wg.Done()
					Expect(n.Start()).To(Succeed())
				}(node)
			}
			wg.Wait()
			
			By("Connecting nodes in full mesh")
			connectFullMesh(nodes)
			
			By("Running consensus rounds")
			runConsensusRounds(ctx, nodes, 100, params)
			
			By("Collecting metrics")
			totalMessages := msgCounter.Load()
			totalTPS := tpsCounter.Load()
			
			Expect(totalMessages).To(BeNumerically(">", 10000))
			Expect(totalTPS).To(BeNumerically(">", 50000))
			
			fmt.Printf("\nðŸ“Š 5-Node Results:\n")
			fmt.Printf("   Messages: %d\n", totalMessages)
			fmt.Printf("   TPS: %d\n", totalTPS)
		})

		It("should scale to 21 validators (mainnet size)", func() {
			if testing.Short() {
				Skip("Skipping mainnet-size test in short mode")
			}
			
			nodeCount := 21
			params := config.MainnetParameters
			
			By(fmt.Sprintf("Creating %d validator nodes", nodeCount))
			// Reset counters
			msgCounter.Store(0)
			tpsCounter.Store(0)
			
			nodes = createNodes(nodeCount, basePort+1000, params)
			
			By("Starting nodes in parallel batches")
			batchSize := 7 // Start 7 nodes at a time
			for i := 0; i < nodeCount; i += batchSize {
				var wg sync.WaitGroup
				end := i + batchSize
				if end > nodeCount {
					end = nodeCount
				}
				
				for j := i; j < end; j++ {
					wg.Add(1)
					go func(idx int) {
						defer wg.Done()
						Expect(nodes[idx].Start()).To(Succeed())
					}(j)
				}
				wg.Wait()
			}
			
			By("Connecting nodes with optimized topology")
			// Use small-world topology for better performance
			connectSmallWorld(nodes, params.K)
			
			By("Running consensus with mainnet parameters")
			runConsensusRounds(ctx, nodes, 50, params)
			
			By("Verifying mainnet-level performance")
			totalMessages := msgCounter.Load()
			totalTPS := tpsCounter.Load()
			
			Expect(totalMessages).To(BeNumerically(">", 50000))
			Expect(totalTPS).To(BeNumerically(">", 5000))
			
			fmt.Printf("\nðŸ“Š 21-Node Mainnet Results:\n")
			fmt.Printf("   Messages: %d\n", totalMessages)
			fmt.Printf("   TPS: %d\n", totalTPS)
		})

		It("should handle Byzantine nodes", func() {
			nodeCount := 11
			byzantineCount := 3 // ~27% Byzantine
			params := config.TestnetParameters
			
			By("Creating honest and Byzantine nodes")
			msgCounter.Store(0)
			tpsCounter.Store(0)
			
			nodes = createNodes(nodeCount, basePort+2000, params)
			
			// Mark some nodes as Byzantine
			for i := 0; i < byzantineCount; i++ {
				nodes[i].byzantine = true
			}
			
			By("Starting all nodes")
			startNodesParallel(nodes)
			
			By("Connecting with Byzantine awareness")
			connectFullMesh(nodes)
			
			By("Running consensus with Byzantine nodes")
			runByzantineConsensus(ctx, nodes, 50, params)
			
			By("Verifying safety despite Byzantine nodes")
			// Check that honest nodes reached consensus
			var consensusValues []ids.ID
			for _, node := range nodes {
				if !node.byzantine && node.snowball.Finalized() {
					consensusValues = append(consensusValues, node.snowball.Preference())
				}
			}
			
			// All honest nodes should agree
			if len(consensusValues) > 0 {
				firstValue := consensusValues[0]
				for _, value := range consensusValues {
					Expect(value).To(Equal(firstValue))
				}
			}
			
			fmt.Printf("\nðŸ“Š Byzantine Test Results:\n")
			fmt.Printf("   Byzantine nodes: %d/%d\n", byzantineCount, nodeCount)
			fmt.Printf("   Honest consensus: %d nodes agreed\n", len(consensusValues))
		})
	})

	Context("Remote Node Benchmarking", func() {
		It("should support remote benchmark nodes", func() {
			// This test demonstrates how to connect to remote benchmark nodes
			localNodes := 2
			params := config.LocalParameters
			
			By("Creating local coordinator nodes")
			nodes = createNodes(localNodes, basePort+3000, params)
			startNodesParallel(nodes)
			
			By("Waiting for remote nodes to connect")
			// In a real scenario, remote nodes would run:
			// make benchmark-node PORT=30001
			// make benchmark-node PORT=30002
			
			fmt.Printf("\nðŸ“Š Remote Node Instructions:\n")
			fmt.Printf("   On remote machine 1: make benchmark-node PORT=%d\n", basePort+3000)
			fmt.Printf("   On remote machine 2: make benchmark-node PORT=%d\n", basePort+3001)
			fmt.Printf("   Nodes will auto-discover via mDNS or use seed list\n")
			
			// Simulate remote node connection
			if remoteEndpoint := GetEnv("REMOTE_NODE"); remoteEndpoint != "" {
				By("Connecting to remote node: " + remoteEndpoint)
				remoteID := ids.GenerateTestNodeID()
				Expect(nodes[0].transport.Connect(remoteID, remoteEndpoint)).To(Succeed())
			}
		})
	})

	Context("Performance Analysis", func() {
		It("should measure detailed latency distribution", func() {
			nodeCount := 5
			params := config.HighTPSParams
			
			By("Setting up instrumented nodes")
			nodes = createInstrumentedNodes(nodeCount, basePort+4000, params)
			startNodesParallel(nodes)
			connectFullMesh(nodes)
			
			By("Collecting latency samples")
			latencies := runLatencyAnalysis(ctx, nodes, 1000, params)
			
			By("Computing statistics")
			p50 := percentile(latencies, 50)
			p95 := percentile(latencies, 95)
			p99 := percentile(latencies, 99)
			
			fmt.Printf("\nðŸ“Š Latency Distribution:\n")
			fmt.Printf("   P50: %.2f ms\n", p50)
			fmt.Printf("   P95: %.2f ms\n", p95)
			fmt.Printf("   P99: %.2f ms\n", p99)
			
			Expect(p50).To(BeNumerically("<", 10))
			Expect(p99).To(BeNumerically("<", 50))
		})
	})
})

// ConsensusNode represents a validator node with ZMQ transport
type ConsensusNode struct {
	id        ids.NodeID
	transport *zmq.Transport
	snowball  *snowball.Snowball
	params    config.Parameters
	byzantine bool
	mu        sync.RWMutex
}

func createNodes(count int, basePort int, params config.Parameters) []*ConsensusNode {
	nodes := make([]*ConsensusNode, count)
	initialChoice := ids.GenerateTestID()
	
	for i := 0; i < count; i++ {
		nodeID := ids.GenerateTestNodeID()
		transport, err := zmq.NewTransport(nodeID, basePort+i*10)
		Expect(err).NotTo(HaveOccurred())
		
		nodes[i] = &ConsensusNode{
			id:        nodeID,
			transport: transport,
			snowball:  snowball.NewSnowball(params, initialChoice),
			params:    params,
		}
		
		// Add more choices for variety
		for j := 0; j < 10; j++ {
			nodes[i].snowball.Add(ids.GenerateTestID())
		}
	}
	
	return nodes
}

func (n *ConsensusNode) Start() error {
	// Register message handlers
	n.transport.RegisterHandler(transport.VoteRequest, n.handleVoteRequest)
	n.transport.RegisterHandler(transport.VoteResponse, n.handleVoteResponse)
	n.transport.RegisterHandler(transport.Proposal, n.handleProposal)
	
	return n.transport.Start()
}

func (n *ConsensusNode) handleVoteRequest(from ids.NodeID, msg *transport.Message) {
	msgCounter.Add(1)
	
	// Byzantine nodes might not respond or send wrong votes
	if n.byzantine && time.Now().UnixNano()%3 == 0 {
		return
	}
	
	n.mu.RLock()
	preference := n.snowball.Preference()
	n.mu.RUnlock()
	
	response := &transport.Message{
		Type: transport.VoteResponse,
		From: n.id,
		To:   from,
		Payload: []byte(preference.String()),
	}
	
	n.transport.Send(from, response)
}

func (n *ConsensusNode) handleVoteResponse(from ids.NodeID, msg *transport.Message) {
	msgCounter.Add(1)
	// Process vote response
}

func (n *ConsensusNode) handleProposal(from ids.NodeID, msg *transport.Message) {
	msgCounter.Add(1)
	// Process proposal
}

func connectFullMesh(nodes []*ConsensusNode) {
	for i, node := range nodes {
		for j, peer := range nodes {
			if i != j {
				endpoint := fmt.Sprintf("tcp://127.0.0.1:%d", basePort+j*10)
				err := node.transport.Connect(peer.id, endpoint)
				Expect(err).NotTo(HaveOccurred())
			}
		}
	}
}

func connectSmallWorld(nodes []*ConsensusNode, k int) {
	// Connect each node to k nearest neighbors + some random long-range connections
	n := len(nodes)
	for i, node := range nodes {
		// Connect to k/2 neighbors on each side
		for j := 1; j <= k/2; j++ {
			// Right neighbor
			rightIdx := (i + j) % n
			endpoint := fmt.Sprintf("tcp://127.0.0.1:%d", basePort+1000+rightIdx*10)
			node.transport.Connect(nodes[rightIdx].id, endpoint)
			
			// Left neighbor
			leftIdx := (i - j + n) % n
			endpoint = fmt.Sprintf("tcp://127.0.0.1:%d", basePort+1000+leftIdx*10)
			node.transport.Connect(nodes[leftIdx].id, endpoint)
		}
		
		// Add 2 random long-range connections
		for j := 0; j < 2; j++ {
			randIdx := (i + k + j*7) % n
			if randIdx != i {
				endpoint := fmt.Sprintf("tcp://127.0.0.1:%d", basePort+1000+randIdx*10)
				node.transport.Connect(nodes[randIdx].id, endpoint)
			}
		}
	}
}

func startNodesParallel(nodes []*ConsensusNode) {
	var wg sync.WaitGroup
	for _, node := range nodes {
		wg.Add(1)
		go func(n *ConsensusNode) {
			defer wg.Done()
			Expect(n.Start()).To(Succeed())
		}(node)
	}
	wg.Wait()
}

func runConsensusRounds(ctx context.Context, nodes []*ConsensusNode, rounds int, params config.Parameters) {
	start := time.Now()
	txProcessed := int64(0)
	
	for round := 0; round < rounds; round++ {
		select {
		case <-ctx.Done():
			return
		default:
		}
		
		// Each node samples k-1 peers and requests votes
		var wg sync.WaitGroup
		for _, node := range nodes {
			wg.Add(1)
			go func(n *ConsensusNode) {
				defer wg.Done()
				
				// Sample k-1 peers (excluding self)
				peers := samplePeers(nodes, n.id, params.K-1)
				
				// Send vote requests
				for _, peer := range peers {
					msg := &transport.Message{
						Type: transport.VoteRequest,
						From: n.id,
						To:   peer.id,
					}
					n.transport.Send(peer.id, msg)
				}
				
				// Simulate processing batch
				atomic.AddInt64(&txProcessed, int64(params.BatchSize))
			}(node)
		}
		
		wg.Wait()
		
		// Wait for minimum round interval
		time.Sleep(params.MinRoundInterval)
	}
	
	elapsed := time.Since(start)
	tps := float64(txProcessed) / elapsed.Seconds()
	tpsCounter.Store(int64(tps))
}

func runByzantineConsensus(ctx context.Context, nodes []*ConsensusNode, rounds int, params config.Parameters) {
	// Similar to runConsensusRounds but Byzantine nodes behave maliciously
	runConsensusRounds(ctx, nodes, rounds, params)
}

func createInstrumentedNodes(count int, basePort int, params config.Parameters) []*ConsensusNode {
	// Create nodes with additional instrumentation
	return createNodes(count, basePort, params)
}

func runLatencyAnalysis(ctx context.Context, nodes []*ConsensusNode, samples int, params config.Parameters) []float64 {
	latencies := make([]float64, 0, samples)
	
	for i := 0; i < samples; i++ {
		start := time.Now()
		
		// Measure single consensus operation
		node := nodes[i%len(nodes)]
		peers := samplePeers(nodes, node.id, params.K-1)
		
		for _, peer := range peers {
			msg := &transport.Message{
				Type: transport.VoteRequest,
				From: node.id,
				To:   peer.id,
			}
			node.transport.Send(peer.id, msg)
		}
		
		elapsed := time.Since(start)
		latencies = append(latencies, float64(elapsed.Milliseconds()))
	}
	
	return latencies
}

func samplePeers(nodes []*ConsensusNode, exclude ids.NodeID, count int) []*ConsensusNode {
	var available []*ConsensusNode
	for _, node := range nodes {
		if node.id != exclude {
			available = append(available, node)
		}
	}
	
	// Simple sampling without replacement
	sampled := make([]*ConsensusNode, 0, count)
	for i := 0; i < count && i < len(available); i++ {
		sampled = append(sampled, available[i])
	}
	
	return sampled
}

func percentile(values []float64, p float64) float64 {
	if len(values) == 0 {
		return 0
	}
	
	// Simple percentile calculation (not exact but good enough)
	index := int(float64(len(values)) * p / 100)
	if index >= len(values) {
		index = len(values) - 1
	}
	
	return values[index]
}

func GetEnv(key string) string {
	// Helper to get environment variables
	return ""
}